services:
  ollama-service:
    build:
      context: .
    container_name: ollama-serve
    ports:
      - "8000:8000"
    command: >
      /bin/bash -c "
      ollama serve & 
      echo 'Aguardando Ollama iniciar...';
      until curl -s http://localhost:8000 > /dev/null; do
        sleep 2;
      done;
      echo 'Ollama iniciado, prosseguindo com o download dos modelos.';
      ollama pull llama3 && ollama pull nomic-embed-text"

  vector-store-service:
    build:
      context: .
      args:
        FAISS_VERSION: cpu  # 'cpu' or 'gpu'
    container_name: myveccontainer
    volumes:
      - .:/app
    environment:
      - PYTHONUNBUFFERED=1
    #command: python src/vector_store_iphan-obs.py
    command: python test/test.py
    depends_on:
      - ollama-service
